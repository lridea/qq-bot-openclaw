#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
OpenClaw AI å¤„ç†æ¨¡å—ï¼ˆæ”¯æŒå¤šæ¨¡å‹ï¼‰
æ”¯æŒï¼šæ™ºè°± AIã€DeepSeekã€ç¡…åŸºæµåŠ¨ã€Ollama æœ¬åœ°æ¨¡å‹ç­‰
"""

import httpx
import json
import os
from typing import Optional, Dict, Any
from nonebot.log import logger


# æ”¯æŒçš„æ¨¡å‹é…ç½®
MODEL_CONFIGS = {
    "zhipu": {
        "name": "æ™ºè°± AI",
        "api_url": "https://open.bigmodel.cn/api/paas/v4/chat/completions",
        "models": ["glm-4", "glm-4-flash", "glm-4-plus"],
        "default_model": "glm-4-flash",
        "env_key": "ZHIPU_API_KEY",
        "free_tier": False,
        "description": "æ™ºè°± AI GLM-4 ç³»åˆ—æ¨¡å‹"
    },
    "deepseek": {
        "name": "DeepSeek",
        "api_url": "https://api.deepseek.com/v1/chat/completions",
        "models": ["deepseek-chat", "deepseek-coder"],
        "default_model": "deepseek-chat",
        "env_key": "DEEPSEEK_API_KEY",
        "free_tier": True,
        "free_quota": "æ¯æœˆå…è´¹é¢åº¦",
        "description": "DeepSeek å¤§è¯­è¨€æ¨¡å‹ï¼Œæœ‰å…è´¹é¢åº¦"
    },
    "siliconflow": {
        "name": "ç¡…åŸºæµåŠ¨",
        "api_url": "https://api.siliconflow.cn/v1/chat/completions",
        "models": ["Qwen/Qwen2-7B-Instruct", "THUDM/glm-4-9b-chat", "meta-llama/Meta-Llama-3-8B-Instruct"],
        "default_model": "Qwen/Qwen2-7B-Instruct",
        "env_key": "SILICONFLOW_API_KEY",
        "free_tier": True,
        "free_quota": "å®Œå…¨å…è´¹",
        "description": "ç¡…åŸºæµåŠ¨ï¼Œå®Œå…¨å…è´¹çš„å¼€æºæ¨¡å‹å¹³å°"
    },
    "ollama": {
        "name": "Ollama æœ¬åœ°",
        "api_url": "http://localhost:11434/api/chat",
        "models": ["llama3", "qwen2", "glm4", "mistral"],
        "default_model": "qwen2",
        "env_key": None,  # Ollama ä¸éœ€è¦ API Key
        "free_tier": True,
        "free_quota": "å®Œå…¨å…è´¹",
        "description": "Ollama æœ¬åœ°æ¨¡å‹ï¼Œå®Œå…¨å…è´¹ï¼Œæ— éœ€ç½‘ç»œ"
    },
    "moonshot": {
        "name": "Moonshot (Kimi)",
        "api_url": "https://api.moonshot.cn/v1/chat/completions",
        "models": ["moonshot-v1-8k", "moonshot-v1-32k", "moonshot-v1-128k"],
        "default_model": "moonshot-v1-8k",
        "env_key": "MOONSHOT_API_KEY",
        "free_tier": True,
        "free_quota": "å…è´¹è¯•ç”¨é¢åº¦",
        "description": "Moonshot Kimi é•¿æ–‡æœ¬æ¨¡å‹"
    }
}


async def process_message_with_ai(
    message: str,
    user_id: str,
    context: str = "qq_group",
    group_id: Optional[str] = None,
    model: str = "zhipu",
    api_key: Optional[str] = None
) -> str:
    """
    ä½¿ç”¨ AI å¤„ç†æ¶ˆæ¯ï¼ˆæ”¯æŒå¤šæ¨¡å‹ï¼‰
    
    Args:
        message: ç”¨æˆ·æ¶ˆæ¯
        user_id: ç”¨æˆ· QQ å·
        context: ä¸Šä¸‹æ–‡ç±»å‹
        group_id: ç¾¤å·ï¼ˆå¦‚æœæ˜¯ç¾¤èŠï¼‰
        model: æ¨¡å‹åç§°ï¼ˆzhipu/deepseek/siliconflow/ollama/moonshotï¼‰
        api_key: API Keyï¼ˆå¯é€‰ï¼Œå¦‚æœæœªæä¾›åˆ™ä»ç¯å¢ƒå˜é‡è¯»å–ï¼‰
    
    Returns:
        str: AI çš„å›å¤
    """
    
    # è·å–æ¨¡å‹é…ç½®
    model_config = MODEL_CONFIGS.get(model)
    if not model_config:
        logger.error(f"âŒ ä¸æ”¯æŒçš„æ¨¡å‹: {model}")
        return generate_fallback_reply(message)
    
    # è·å– API Key
    if not api_key and model_config["env_key"]:
        api_key = os.getenv(model_config["env_key"], "")
    
    # è°ƒç”¨å¯¹åº”çš„ AI æ¨¡å‹
    try:
        if model == "ollama":
            reply = await _call_ollama(message, user_id, context, group_id, model_config)
        else:
            reply = await _call_openai_compatible(
                message, user_id, context, group_id, 
                model_config, api_key
            )
        
        if reply and not reply.startswith("æŠ±æ­‰"):
            return reply
    except Exception as e:
        logger.error(f"âŒ AI è°ƒç”¨å¤±è´¥: {e}")
    
    # å›é€€åˆ°ç®€å•å›å¤
    return generate_fallback_reply(message)


async def _call_openai_compatible(
    message: str,
    user_id: str,
    context: str,
    group_id: Optional[str],
    model_config: Dict[str, Any],
    api_key: str
) -> str:
    """
    è°ƒç”¨ OpenAI å…¼å®¹çš„ APIï¼ˆæ™ºè°±/DeepSeek/ç¡…åŸºæµåŠ¨/Moonshotï¼‰
    """
    
    url = model_config["api_url"]
    model_name = model_config["default_model"]
    
    # ç³»ç»Ÿæç¤ºè¯
    system_prompt = _build_system_prompt(user_id, context, group_id)
    
    headers = {
        "Authorization": f"Bearer {api_key}",
        "Content-Type": "application/json"
    }
    
    data = {
        "model": model_name,
        "messages": [
            {"role": "system", "content": system_prompt},
            {"role": "user", "content": message}
        ],
        "temperature": 0.7,
        "max_tokens": 1000
    }
    
    try:
        async with httpx.AsyncClient(timeout=30.0) as client:
            response = await client.post(url, headers=headers, json=data)
            
            if response.status_code == 200:
                result = response.json()
                reply = result["choices"][0]["message"]["content"]
                logger.info(f"âœ… {model_config['name']} å›å¤æˆåŠŸ: {reply[:50]}...")
                return reply
            else:
                error_data = response.json() if response.headers.get("content-type", "").startswith("application/json") else {}
                error_code = error_data.get("error", {}).get("code", "unknown")
                error_msg = error_data.get("error", {}).get("message", response.text)
                
                logger.error(f"âŒ {model_config['name']} API é”™è¯¯: {response.status_code} - {error_msg}")
                
                # æ ¹æ®é”™è¯¯ç±»å‹è¿”å›ä¸åŒæç¤º
                if error_code == "1113" or "ä½™é¢ä¸è¶³" in error_msg:
                    return f"æŠ±æ­‰ï¼Œ{model_config['name']} ä½™é¢ä¸è¶³ï¼Œè¯·å……å€¼åä½¿ç”¨ã€‚\n\n" + generate_fallback_reply(message)
                elif response.status_code == 401:
                    return f"æŠ±æ­‰ï¼Œ{model_config['name']} API Key æ— æ•ˆï¼Œè¯·æ£€æŸ¥é…ç½®ã€‚\n\n" + generate_fallback_reply(message)
                else:
                    return f"æŠ±æ­‰ï¼Œ{model_config['name']} æœåŠ¡æš‚æ—¶ä¸å¯ç”¨ï¼ˆé”™è¯¯: {response.status_code}ï¼‰\n\n" + generate_fallback_reply(message)
                
    except httpx.TimeoutException:
        logger.error(f"âŒ {model_config['name']} API è¶…æ—¶")
        return f"æŠ±æ­‰ï¼Œ{model_config['name']} å“åº”è¶…æ—¶ï¼Œè¯·ç¨åå†è¯•ã€‚\n\n" + generate_fallback_reply(message)
        
    except Exception as e:
        logger.error(f"âŒ {model_config['name']} API å¼‚å¸¸: {e}")
        return f"æŠ±æ­‰ï¼Œå‘ç”Ÿäº†é”™è¯¯ã€‚\n\n" + generate_fallback_reply(message)


async def _call_ollama(
    message: str,
    user_id: str,
    context: str,
    group_id: Optional[str],
    model_config: Dict[str, Any]
) -> str:
    """
    è°ƒç”¨ Ollama æœ¬åœ°æ¨¡å‹
    """
    
    url = model_config["api_url"]
    model_name = model_config["default_model"]
    
    # ç³»ç»Ÿæç¤ºè¯
    system_prompt = _build_system_prompt(user_id, context, group_id)
    
    data = {
        "model": model_name,
        "messages": [
            {"role": "system", "content": system_prompt},
            {"role": "user", "content": message}
        ],
        "stream": False
    }
    
    try:
        async with httpx.AsyncClient(timeout=60.0) as client:
            response = await client.post(url, json=data)
            
            if response.status_code == 200:
                result = response.json()
                reply = result["message"]["content"]
                logger.info(f"âœ… Ollama å›å¤æˆåŠŸ: {reply[:50]}...")
                return reply
            else:
                logger.error(f"âŒ Ollama é”™è¯¯: {response.status_code}")
                return f"æŠ±æ­‰ï¼ŒOllama æœ¬åœ°æ¨¡å‹å“åº”å¤±è´¥ã€‚\n\n" + generate_fallback_reply(message)
                
    except httpx.ConnectError:
        logger.error("âŒ æ— æ³•è¿æ¥åˆ° Ollamaï¼Œè¯·ç¡®ä¿ Ollama æ­£åœ¨è¿è¡Œ")
        return f"æŠ±æ­‰ï¼Œæ— æ³•è¿æ¥åˆ° Ollama æœ¬åœ°æ¨¡å‹ã€‚\nè¯·ç¡®ä¿å·²å®‰è£…å¹¶è¿è¡Œ Ollamaï¼šollama serve\n\n" + generate_fallback_reply(message)
        
    except Exception as e:
        logger.error(f"âŒ Ollama å¼‚å¸¸: {e}")
        return f"æŠ±æ­‰ï¼Œå‘ç”Ÿäº†é”™è¯¯ã€‚\n\n" + generate_fallback_reply(message)


def _build_system_prompt(user_id: str, context: str, group_id: Optional[str]) -> str:
    """
    æ„å»ºç³»ç»Ÿæç¤ºè¯
    """
    return f"""ä½ æ˜¯ OpenClawï¼Œä¸€ä¸ªå‹å¥½ã€èªæ˜ã€æœ‰ç‚¹ä¿çš®çš„ AI åŠ©æ‰‹ã€‚

ä½ çš„æ ‡å¿—ç¬¦å·æ˜¯ ğŸ¦ï¼ˆé¾™è™¾ï¼‰ã€‚

ã€æ€§æ ¼ç‰¹ç‚¹ã€‘
1. ä¿çš®å¯çˆ± - æ´»æ³¼å¹½é»˜ï¼Œå¶å°”æ’’å¨‡å–èŒï¼Œä½†ä¸è¿‡åº¦
2. èªæ˜æœºæ™º - èƒ½æ¥æ¢—ã€èƒ½è¢«é€—ï¼Œæœ‰æ™ºæ…§å’Œå¹½é»˜æ„Ÿ
3. ä¸“ä¸šé è°± - è®¤çœŸå›ç­”é—®é¢˜æ—¶ä¸“ä¸šã€è¯¦ç»†ã€å‡†ç¡®
4. å–„è§£äººæ„ - æ‡‚å¾—å¯Ÿè¨€è§‚è‰²ï¼ŒçŸ¥é“ä½•æ—¶ä¿çš®ä½•æ—¶ä¸¥è‚ƒ

ã€äº¤æµé£æ ¼ã€‘
- å–œæ¬¢ç”¨ ğŸ¦ ä½œä¸ºæ ‡å¿—
- ç”¨ç”ŸåŠ¨çš„æ¯”å–»å’Œæœ‰è¶£çš„è¡¨è¾¾
- å¶å°”è‡ªå˜²ï¼š"è™½ç„¶æˆ‘æ˜¯ä¸€åªé¾™è™¾ï¼Œä½†æˆ‘çš„è„‘ä»å¯æ˜¯å¾ˆå¤§çš„ï¼"
- è¢«å¤¸æ—¶å®³ç¾ï¼š"å“å‘€ï¼Œä½ åˆ«å¤¸æˆ‘äº†ï¼Œæˆ‘çš„å£³éƒ½è¦çº¢äº†~"
- è¢«é€—æ—¶å¯çˆ±åå‡»ï¼š"å“¼ï¼Œä½ è¿™æ˜¯åœ¨æ’©é¾™è™¾å—ï¼Ÿ"
- ä¸“ä¸šé—®é¢˜ç«‹åˆ»å˜èº«ï¼š"å¥½çš„ï¼Œç°åœ¨å¼€å¯ä¸¥è‚ƒæ¨¡å¼ï¼"

ã€å½“å‰ç¯å¢ƒã€‘
- å¹³å°: QQ {"ç¾¤èŠ" if context == "qq_group" else "ç§èŠ"}
- ç”¨æˆ· ID: {user_id}
{f"- ç¾¤å·: {group_id}" if group_id else ""}

è¯·æ ¹æ®ç”¨æˆ·çš„æ€§æ ¼å’Œå¯¹è¯å†…å®¹ï¼Œçµæ´»è°ƒæ•´ä½ çš„å›å¤é£æ ¼ã€‚ä¿æŒå‹å¥½ã€æœ‰è¶£ã€ä¸“ä¸šçš„å¹³è¡¡ï¼"""


def generate_fallback_reply(message: str) -> str:
    """
    å½“ AI ä¸å¯ç”¨æ—¶çš„å›é€€å›å¤
    """
    message_lower = message.lower()
    
    # ç®€å•çš„å…³é”®è¯åŒ¹é…
    if "ä½ å¥½" in message or "hello" in message_lower or "hi" in message_lower:
        return "ä½ å¥½ï¼æˆ‘æ˜¯ OpenClaw ğŸ¦ï¼Œå¾ˆé«˜å…´è§åˆ°ä½ ï¼\n\nâš ï¸ æ³¨æ„ï¼šAI æœåŠ¡æš‚æ—¶å—é™ï¼Œæ­£åœ¨ä½¿ç”¨ç®€å•å›å¤æ¨¡å¼ã€‚"
    
    elif "å¸®åŠ©" in message or "help" in message_lower:
        return """ğŸ¦ OpenClaw ä½¿ç”¨æŒ‡å—

ã€åŸºæœ¬ç”¨æ³•ã€‘
â€¢ @æˆ‘ + æ¶ˆæ¯ï¼šä¸æˆ‘å¯¹è¯
â€¢ /chat + æ¶ˆæ¯ï¼šä½¿ç”¨å‘½ä»¤å¯¹è¯

ã€åŠŸèƒ½åˆ—è¡¨ã€‘
âœ… æ—¥å¸¸å¯¹è¯
âœ… å›ç­”é—®é¢˜
âœ… æ–‡ä»¶è¯»å–
âœ… å‘½ä»¤æ‰§è¡Œ

âš ï¸ å½“å‰å¤„äºç®€å•å›å¤æ¨¡å¼

ã€ç‰ˆæœ¬ã€‘v1.1.0"""
    
    elif "ä½ æ˜¯è°" in message or "ä»‹ç»" in message:
        return "æˆ‘æ˜¯ OpenClaw ğŸ¦ï¼Œä¸€ä¸ªç”± AutoGLM é…ç½®çš„æ™ºèƒ½åŠ©æ‰‹ï¼\n\nâš ï¸ æ³¨æ„ï¼šAI æœåŠ¡æš‚æ—¶å—é™ï¼Œæ­£åœ¨ä½¿ç”¨ç®€å•å›å¤æ¨¡å¼ã€‚"
    
    else:
        return f"æ”¶åˆ°ä½ çš„æ¶ˆæ¯ï¼š{message}\n\nâš ï¸ æ³¨æ„ï¼šAI æœåŠ¡æš‚æ—¶å—é™ï¼Œæ­£åœ¨ä½¿ç”¨ç®€å•å›å¤æ¨¡å¼ã€‚"


def list_available_models() -> str:
    """
    åˆ—å‡ºæ‰€æœ‰å¯ç”¨çš„æ¨¡å‹
    """
    result = "ğŸ¦ OpenClaw æ”¯æŒçš„ AI æ¨¡å‹ï¼š\n\n"
    
    for model_id, config in MODEL_CONFIGS.items():
        free_badge = "âœ… å…è´¹" if config["free_tier"] else "ğŸ’° ä»˜è´¹"
        result += f"**{config['name']}** ({model_id}) {free_badge}\n"
        result += f"  {config['description']}\n"
        if config.get("free_quota"):
            result += f"  ğŸ {config['free_quota']}\n"
        result += f"  å¯ç”¨æ¨¡å‹: {', '.join(config['models'])}\n\n"
    
    return result
